
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators</title>

    <meta name="description" content="Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://rl-at-scale.github.io/img/rls-teaser.jpg">
    <meta property="og:image:type" content="image/jpeg">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://rl-at-scale.github.io/"/>
    <meta property="og:title" content="Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators" />
    <meta property="og:description" content="Project page for Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators" />
    <meta name="twitter:description" content="Project page for Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators." />
    <meta name="twitter:image" content="https://rl-at-scale.github.io/img/rls-teaser.jpg" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+6">Deep RL at Scale: </br> Sorting Waste in Office Buildings </br>with a Fleet of Mobile Manipulators </font></strong> </br> 
                <!--<small>
                    CoRL 2021
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li>Alexander Herzog*</li> <li>Kanishka Rao*</li> <li>Karol Hausman*</li> <li>Yao Lu*</li> <li>Paul Wohlhart*</li> <br>
                <li>Mengyuan Yan</li> <li>Jessica Lin</li> <li>Montserrat Gonzalez Arenas</li> <li>Ted Xiao</li> <li>Daniel Kappler</li> <li>Daniel Ho</li> <br> 
                <li>Jarek Rettinghouse</li> <li>Yevgen Chebotar</li> <li>Kuang-Huei Lee</li> <li>Keerthana Gopalakrishnan</li> <li>Ryan Julian</li> <li>Adrian Li</li> <br>
               <li>Chuyuan Kelly Fu</li> <li>Bob Wei</li> <li>Sangeetha Ramesh</li> <li>Khem Holden</li> <li>Kim Kleiven</li> <li>David Rendleman</li> <br>
                <li>Sean Kirmani</li> <li>Jeff Bingham</li> <li>Jon Weisz</li> <li>Ying Xu</li> <li>Wenlong Lu</li> <li>Matthew Bennice</li> <li>Cody Fong</li> <li>David Do</li>  <br>
                <li>Jessica Lam</li> <li>Noah Brown</li> <li>Mrinal Kalakrishnan</li> <li>Julian Ibarz</li> <li>Peter Pastor</li> <li>Sergey Levine</li> <br>
                <br>
			*Authors with equal contribution
            <br>
             <a href="https://everydayrobots.com"> <image src="img/edr-logo.png" height="40px"> </a>
             <a href="http://g.co/robotics"> <image src="img/rng-logo.png" height="37px"> </a>
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="assets/rl_at_scale.pdf">
                        <image src="img/paper.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>

                    <li>
                        <a href="https://youtu.be/CHS9HSb1uqA">
                        <image src="img/youtube_icon.png" height="60px">
                            <h4><strong>Video</strong></h4>
                        </a>
                    </li>
                    <!-- <li>
                        <a href="http://ai.googleblog.com/2023/04/rt-1-robotics-transformer-for-real.html">
                        <image src="img/google-ai-blog-small.png" height="60px">
                            <h4><strong>Blogpost</strong></h4>
                        </a>
                    </li>
                        <li>
                        <a href="https://github.com/google-research/robotics_transformer">
                        <image src="img/github.png" height="60px">
                            <h4><strong>Code</strong></h4>
                        </a>                   
                    </li>  -->
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                 <p style="text-align:center;">
            	    <video id="arch" width="100%" playsinline="" autoplay="" muted="" loop="">
                           <source src="videos/sort_at_rls_1440x810.mp4" type="video/mp4">
                       </video>
                </p>
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                   We describe a system for deep reinforcement learning of robotic manipulation skills applied to a large-scale real-world task: sorting recyclables and trash in office buildings. Real-world deployment of deep RL policies requires not only effective training algorithms, but the ability to bootstrap real-world training and enable broad generalization. To this end, our system - RL at Scale (RLS) - combines scalable deep RL from real-world data with bootstrapping from training in simulation, and incorporates auxiliary inputs from existing computer vision systems as a way to boost generalization to novel objects, while retaining the benefits of end-to-end training.
                   We analyze the tradeoffs of different design decisions in our system, and present a large-scale empirical validation that includes training on real-world data gathered over the course of 24 months of experimentation, across a fleet of 23 robots in three office buildings, with a total training set of 9527 hours of robotic experience. Our final validation also consists of 4800 evaluation trials across 240 waste station configurations, in order to evaluate in detail the impact of the design decisions in our system, the scaling effects of including more real-world data, and the performance of the method on novel objects.
                </p>
            </div>
        </div>   

    	<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/CHS9HSb1uqA" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Problem Setup
                </h3>
                We study the problem of continual real-world reinforcement learning through the lenses of a large scale experiment, where we deployed a fleet of 23 RL-enabled robots over two years in Google office buildings to sort waste and recycling. 
                In our experiment, a robot roamed around an office building searching for “waste stations” (bins for recyclables, compost, and trash). The robot was tasked with approaching each waste station to sort it, moving items between the bins so that all recyclables (cans, bottles, etc.) were placed in the recyclable bin, all the compostable items (cardboard containers, paper cups, etc.) were placed in the compost bin, and everything else was placed in the landfill trash bin.<br> <br>
                The task of sorting waste is much harder than it sounds: not only does the robot need to correctly pick up the vast variety of objects that people deposit into waste bins, but it also needs to identify the appropriate bin for each object and sort them as quickly and efficiently as possible. <br><br>
                <p style="text-align:center;">
                <video id="v0" width="100%" playsinline autoplay muted loop>
                   <source src="videos/sorty_task_animation.mp4" type="video/mp4">
               </video>
               </p>
The experiment setup enabled robots to learn on the job and improve through real-world experience, additional autonomous data collection in “robot classrooms,” and simulation. Our robotic system combines scalable deep RL from real-world data with bootstrapping from training in simulation and auxiliary object perception inputs to boost generalization, while retaining the benefits of end-to-end training, which we validate with 4,800 evaluation trials across 240 waste station configurations.                
		<p class="text-justify">

            </div>
        </div>
        
         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Bootstrapping
                </h3>
          <p style="text-align:center;">
        	    <image src="img/flywheel.png" class="img-responsive">        	   
         </p>       
         To make sure that robots can learn on the job, we need to bootstrap the robots with a basic set of skills. To this end, we use four sources of experience: (1) a set of simple hand-designed policies that have a very low success rate, but serve to provide some initial experience, (2) a simulated training framework that uses sim-to-real transfer to provide some initial bin sorting strategies, (3) robot classrooms where the robots continually practice at a set of representative waste stations, and (4) the real deployment setting, where robots practice in real office buildings with real trash. <br><br>
   
We start with learning sorting in simulation using a previously-developed PI-QT-Opt framework to obtain the sorting policy. To make the sim2real possible, we apply separately-trained RetinaGAN to make the simulated images look closer to reality as shown below. <br><br>
                <p style="text-align:center;">
                <video id="v0" width="100%" playsinline autoplay muted loop>
                   <source src="videos/sorty_sim_to_real.mp4" type="video/mp4">
               </video>
               </p>
		<p class="text-justify">
Once we have an initial sim2real policy and data collected using scripts in the real world, we are off to collecting data autonomously in a lab setting which we call a "robot classroom". 
While real-world office buildings can provide the most representative experience, the throughput in terms of data collection is limited – some days there will be a lot of trash to sort, some days not so much. Our robots collect a large portion of their experience in “robot classrooms.” In the classroom shown below, 20 robots practice the waste sorting task:
                <p style="text-align:center;">
                <video id="v0" width="100%" playsinline autoplay muted loop>
                   <source src="videos/classrooms_wide.mp4" type="video/mp4">
               </video>
               </p>
Equipped with the data coming from scripts, simulation and robot classroom, we continuously train our waste sorting policies using PI-QT-Opt. The resulting policy is deployed in the real office buildings - in this case we deployed RLS at 3 office buildings with 30 waste stations. <br><br>
                <p style="text-align:center;">
                <video id="v0" width="100%" playsinline autoplay muted loop>
                   <source src="videos/sorty_wild_collage_flat.mp4" type="video/mp4">
               </video>
               </p>                              
The resulting policy was continually trained using all sources of data to continuously improve sorting success in novel scenarios.
            </div>
        </div>
                
                


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
               
               Equipped with real and simulated data, we use deep RL to train an end-to-end policy that is directly optimized for reducing the contamination of the bins. 
Similarly to how we train our simulation policy, we use PI-QT-Opt to train the final policy on the complete dataset assembled from simulation and real world collection.

<br><br>
The diagram of the neural network architecture of the Q-function that is learned with PI-QT-Opt is shown below. 
          <p style="text-align:center;">
        	    <image src="img/architecture.png" class="img-responsive">        	   
         </p>          
We feed two RGB images to two separate convolutional towers which are later concatenated and processed by another set of convolutional layers. The two images correspond to the current camera image as well as the object mask image. The object mask image is an extra image with a dot at the center of every object that is currently misplaced. The color of the dot indicates which bin the object should be sorted into and is trained using a pre-trained vision model. This image is fed to the network as an extra input channel concatenated to the current RGB image.
<br><br>
We train this model using Deep RL, which allows us to not only distill the best possible policy out of the bootstrapping data, but also to enable the robot to improve continuously as it interacts with waste stations more and more.                

            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                In the end, we gathered 540k trials in the classrooms and 32.5k trials from deployment. Overall system performance improved as more data was collected. We evaluated our final system in the classrooms to allow for controlled comparisons, setting up scenarios based on what the robots saw during deployment. You can see the classroom evaluation scenes below. 

          <p style="text-align:center;">
        	    <image src="img/classroom-eval.png" class="img-responsive">        	   
         </p>          

The final system could accurately sort about 84% of the objects on average, with performance increasing steadily as more data was added. We performed an ablation study to understand how our design decisions contribute to the final performance as seen in the next graph.  
          <p style="text-align:center;">
        	    <image src="img/classroom-result.png" class="img-responsive">        	   
         </p>          
  
Lastly, one of the most challenging aspects of this problem was the diversity of data encountered in the real office buildings. We present a few examples with a variety of situations with unique objects from real office buildings below.<br><br>
    <p style="text-align:center;">
        	    <image src="img/diverse.png" class="img-responsive">        	   
    </p>
    In the real world, we logged statistics from three real-world deployments between 2021 and 2022, and found that our system could reduce contamination in the waste bins by between 40% and 50% despite the challenging out-of-distribution scenarios. 
    <p style="text-align:center;">
          <image src="img/deployment.png" class="img-responsive">        	   
   </p> 
Our paper provides further insights on the technical design, ablations studying various design decisions, and more detailed statistics on the experiments.
            </div>
        </div>
             

           

	<!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/CHS9HSb1uqA" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->



         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation 
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{rlscale2023arxiv,
    title={Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators},
    author={Alexander Herzog* and Kanishka Rao* and Karol Hausman* and Yao Lu* and Paul Wohlhart* and Mengyuan Yan and Jessica Lin and Montserrat Gonzalez Arenas and Ted Xiao and Daniel Kappler and
Daniel Ho and Jarek Rettinghouse and Yevgen Chebotar and Kuang-Huei Lee and Keerthana Gopalakrishnan and Ryan Julian and Adrian Li and Chuyuan Kelly Fu and Bob Wei and Sangeetha Ramesh and Khem Holden and Kim Kleiven and David Rendleman and Sean Kirmani and Jeff Bingham and Jon Weisz and Ying Xu and
Wenlong Lu and Matthew Bennice and Cody Fong and David Do and Jessica Lam and Noah Brown and Mrinal Kalakrishnan and Julian Ibarz and Peter Pastor and Sergey Levine},
    booktitle={arXiv preprint arXiv:TODO},
    year={2023}
}</textarea>
                </div>
            </div>
             
        </div>


         <!-- <div class="row">
            <div id="open-source" class="col-md-8 col-md-offset-2">
                <h3>
                    Open Source
                </h3>
              We open source the RL@Scale model <a href="https://github.com/google-research/rl_at_scale">[here]. </a>
              <p style="text-align:center;">
                </p>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    We would like to thank Mohi Khansari, Cameron Tuckerman, Stanley Soo, Benjie Holson, Justin Vincent, Mario Prats, Thomas Buschmann, Michael Quinlan, Yunfei Bai, Joséphine Simon, Jarrett Lee, Kalpesh Kuber, Meghha Dhoke, Christian Bodner and Russell Wong for their help and support in various aspects of the project.
                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
