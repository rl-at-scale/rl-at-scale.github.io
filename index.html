
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators</title>

    <meta name="description" content="Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://rl-at-scale.github.io/img/rls-teaser.jpeg">
    <meta property="og:image:type" content="image/jpeg">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://rl-at-scale.github.io/"/>
    <meta property="og:title" content="Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators" />
    <meta property="og:description" content="Project page for Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators" />
    <meta name="twitter:description" content="Project page for Deep RL at Scale: Sorting Waste in Office Buildings with a Fleet of Mobile Manipulators." />
    <meta name="twitter:image" content="https://rl-at-scale.github.io/img/rls-teaser.jpeg" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+6">Deep RL at Scale: </br> Sorting Waste in Office Buildings </br>with a Fleet of Mobile Manipulators </font></strong> </br> 
                <!--<small>
                    CoRL 2021
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                <br>
                <li>Alexander Herzog*&dagger;</li> <li>Kanishka Rao*&Dagger;</li> <li>Karol Hausman*&Dagger;</li> <li>Yao Lu*&Dagger;</li> <li>Paul Wohlhart*&dagger;</li> <br>
                <li>Mengyuan Yan&dagger;</li> <li>Jessica Lin&dagger;</li> <li>Montserrat Gonzalez Arenas&Dagger;</li> <li>Ted Xiao&Dagger;</li> <li>Daniel Kappler&dagger;</li> <li>Daniel Ho&dagger;</li> <br> 
                <li>Jarek Rettinghouse&dagger;</li> <li>Yevgen Chebotar&Dagger;</li> <li>Kuang-Huei Lee&Dagger;</li> <li>Keerthana Gopalakrishnan&Dagger;</li> <li>Ryan Julian&Dagger;</li> <li>Adrian Li&dagger;</li> <br>
               <li>Chuyuan Kelly Fu&dagger;</li> <li>Bob Wei&dagger;</li> <li>Sangeetha Ramesh&dagger;</li> <li>Khem Holden&Dagger;</li> <li>Kim Kleiven&dagger;</li> <li>David Rendleman&Dagger;</li> <br>
                <li>Sean Kirmani&dagger;</li> <li>Jeff Bingham&dagger;</li> <li>Jon Weisz&dagger;</li> <li>Ying Xu&dagger;</li> <li>Wenlong Lu&dagger;</li> <li>Matthew Bennice&dagger;</li> <li>Cody Fong&dagger;</li> <li>David Do&dagger;</li>  <br>
                <li>Jessica Lam&dagger;</li> <li>Noah Brown&Dagger;</li> <li>Mrinal Kalakrishnan&dagger;</li> <li>Julian Ibarz&Dagger;</li> <li>Peter Pastor&dagger;</li> <li>Sergey Levine&Dagger;</li> <br>
                <br>
			*Authors with equal contribution
            <br>
            &dagger; <a href="https://everydayrobots.com"> <image src="img/edr-logo.png" height="40px"> </a>
            &Dagger; <a href="http://g.co/robotics"> <image src="img/rng-logo.png" height="37px"> </a>
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="assets/rl_at_scale.pdf">
                        <image src="img/paper.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>

                    <li>
                        <a href="https://youtu.be/xyvdGlvwSDE">
                        <image src="img/youtube_icon.png" height="60px">
                            <h4><strong>Video</strong></h4>
                        </a>
                    </li>
                    <!-- <li>
                        <a href="http://ai.googleblog.com/2023/04/rt-1-robotics-transformer-for-real.html">
                        <image src="img/google-ai-blog-small.png" height="60px">
                            <h4><strong>Blogpost</strong></h4>
                        </a>
                    </li>
                        <li>
                        <a href="https://github.com/google-research/robotics_transformer">
                        <image src="img/github.png" height="60px">
                            <h4><strong>Code</strong></h4>
                        </a>                   
                    </li>  -->
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                 <p style="text-align:center;">
            	    <video id="arch" width="100%" playsinline="" autoplay="" muted="" loop="">
                           <source src="videos/sort_at_rls_1440x810.mp4" type="video/mp4">
                       </video>
                </p>
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                   We describe a system for deep reinforcement learning of robotic manipulation skills applied to a large-scale real-world task: sorting recyclables and trash in office buildings. Real-world deployment of deep RL policies requires not only effective training algorithms, but the ability to bootstrap real-world training and enable broad generalization. To this end, our system combines scalable deep RL from real-world data with bootstrapping from training in simulation, and incorporates auxiliary inputs from existing computer vision systems as a way to boost generalization to novel objects, while retaining the benefits of end-to-end training.
                   We analyze the tradeoffs of different design decisions in our system, and present a large-scale empirical validation that includes training on real-world data gathered over the course of 24 months of experimentation, across a fleet of 23 robots in three office buildings, with a total training set of 9527 hours of robotic experience. Our final validation also consists of 4800 evaluation trials across 240 waste station configurations, in order to evaluate in detail the impact of the design decisions in our system, the scaling effects of including more real-world data, and the performance of the method on novel objects.
                </p>
            </div>
        </div>   

    	<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/xyvdGlvwSDE" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Problem Setup
                </h3>
                The list of instructions was designed to show multiple skills with many objects to test aspects of RT-1 such as generalization to new instructions and ability to perform many skills. 
The entire process of adding tasks and data is described in detail in the paper.
Since we do not make any assumptions about particular skills when adding new instructions, the system is easily extendable, and we can continuously provide more diverse data to improve its capabilities.
                <p style="text-align:center;">
                <video id="v0" width="100%" playsinline autoplay muted loop>
                   <source src="videos/sorty_task_animation.mp4" type="video/mp4">
               </video>
               </p>
		<p class="text-justify">
To test RT-1 in the real world, we collected a large dataset of real-world robotic experiences that consists of over 130k episodes, which contain over 700 tasks, and was collected with a fleet of 13 robots over 17 months.
<br><br>
The current set of skills includes picking, placing, opening and closing drawers, getting items in and out drawers, placing elongated items up-right, knocking them over, pulling napkins and opening jars. 
The list of instructions was designed to show multiple skills with many objects to test aspects of RT-1 such as generalization to new instructions and ability to perform many skills. 
The entire process of adding tasks and data is described in detail in the paper.
Since we do not make any assumptions about particular skills when adding new instructions, the system is easily extendable, and we can continuously provide more diverse data to improve its capabilities.
            </div>
        </div>
        
         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Bootstrapping
                </h3>
          <p style="text-align:center;">
        	    <image src="img/flywheel.png" class="img-responsive">        	   
         </p>          

                
                The list of instructions was designed to show multiple skills with many objects to test aspects of RT-1 such as generalization to new instructions and ability to perform many skills. 
The entire process of adding tasks and data is described in detail in the paper.
Since we do not make any assumptions about particular skills when adding new instructions, the system is easily extendable, and we can continuously provide more diverse data to improve its capabilities.
                <p style="text-align:center;">
                <video id="v0" width="100%" playsinline autoplay muted loop>
                   <source src="videos/sorty_sim_to_real.mp4" type="video/mp4">
               </video>
               </p>
		<p class="text-justify">
To test RT-1 in the real world, we collected a large dataset of real-world robotic experiences that consists of over 130k episodes, which contain over 700 tasks, and was collected with a fleet of 13 robots over 17 months.
                <p style="text-align:center;">
                <video id="v0" width="100%" playsinline autoplay muted loop>
                   <source src="videos/classrooms_wide.mp4" type="video/mp4">
               </video>
               </p>
To test RT-1 in the real world, we collected a large dataset of real-world robotic experiences that consists of over 130k episodes, which contain over 700 tasks, and was collected with a fleet of 13 robots over 17 months.
                <p style="text-align:center;">
                <video id="v0" width="100%" playsinline autoplay muted loop>
                   <source src="videos/sorty_wild_collage_flat.mp4" type="video/mp4">
               </video>
               </p>                              
               
<br><br>

The current set of skills includes picking, placing, opening and closing drawers, getting items in and out drawers, placing elongated items up-right, knocking them over, pulling napkins and opening jars. 
The list of instructions was designed to show multiple skills with many objects to test aspects of RT-1 such as generalization to new instructions and ability to perform many skills. 
The entire process of adding tasks and data is described in detail in the paper.
Since we do not make any assumptions about particular skills when adding new instructions, the system is easily extendable, and we can continuously provide more diverse data to improve its capabilities.
            </div>
        </div>
                
                


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
          <p style="text-align:center;">
        	    <image src="img/architecture.png" class="img-responsive">        	   
         </p>          

                
                The list of instructions was designed to show multiple skills with many objects to test aspects of RT-1 such as generalization to new instructions and ability to perform many skills. 
The entire process of adding tasks and data is described in detail in the paper.
Since we do not make any assumptions about particular skills when adding new instructions, the system is easily extendable, and we can continuously provide more diverse data to improve its capabilities.
To test RT-1 in the real world, we collected a large dataset of real-world robotic experiences that consists of over 130k episodes, which contain over 700 tasks, and was collected with a fleet of 13 robots over 17 months.
To test RT-1 in the real world, we collected a large dataset of real-world robotic experiences that consists of over 130k episodes, which contain over 700 tasks, and was collected with a fleet of 13 robots over 17 months.
               
<br><br>
The current set of skills includes picking, placing, opening and closing drawers, getting items in and out drawers, placing elongated items up-right, knocking them over, pulling napkins and opening jars. 
The list of instructions was designed to show multiple skills with many objects to test aspects of RT-1 such as generalization to new instructions and ability to perform many skills. 
The entire process of adding tasks and data is described in detail in the paper.
Since we do not make any assumptions about particular skills when adding new instructions, the system is easily extendable, and we can continuously provide more diverse data to improve its capabilities.
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
          <p style="text-align:center;">
        	    <image src="img/classroom-eval.png" class="img-responsive">        	   
         </p>          

                
                The list of instructions was designed to show multiple skills with many objects to test aspects of RT-1 such as generalization to new instructions and ability to perform many skills. 
The entire process of adding tasks and data is described in detail in the paper.
Since we do not make any assumptions about particular skills when adding new instructions, the system is easily extendable, and we can continuously provide more diverse data to improve its capabilities.
To test RT-1 in the real world, we collected a large dataset of real-world robotic experiences that consists of over 130k episodes, which contain over 700 tasks, and was collected with a fleet of 13 robots over 17 months.
To test RT-1 in the real world, we collected a large dataset of real-world robotic experiences that consists of over 130k episodes, which contain over 700 tasks, and was collected with a fleet of 13 robots over 17 months.

          <p style="text-align:center;">
        	    <image src="img/classroom-result.png" class="img-responsive">        	   
         </p>          
               
<br><br>
The current set of skills includes picking, placing, opening and closing drawers, getting items in and out drawers, placing elongated items up-right, knocking them over, pulling napkins and opening jars. 
          <p style="text-align:center;">
        	    <image src="img/deployment.png" class="img-responsive">        	   
         </p>   
The list of instructions was designed to show multiple skills with many objects to test aspects of RT-1 such as generalization to new instructions and ability to perform many skills. 
The entire process of adding tasks and data is described in detail in the paper.
    <p style="text-align:center;">
        	    <image src="img/diverse.png" class="img-responsive">        	   
    </p>

Since we do not make any assumptions about particular skills when adding new instructions, the system is easily extendable, and we can continuously provide more diverse data to improve its capabilities.
            </div>
        </div>
             

           

	<!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe width="560" height="315" src="https://youtu.be/xyvdGlvwSDE" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->



         <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation 
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{rt12022arxiv,
    title={RT-1: Robotics Transformer for Real-World Control at Scale},
    author={Anthony	Brohan and  Noah Brown and  Justice Carbajal and  Yevgen Chebotar and  Joseph Dabis and  Chelsea Finn and  Keerthana Gopalakrishnan and  Karol Hausman and  Alex Herzog and  Jasmine Hsu and  Julian Ibarz and  Brian Ichter and  Alex Irpan and  Tomas Jackson and  Sally Jesmonth and  Nikhil Joshi and  Ryan Julian and  Dmitry Kalashnikov and  Yuheng Kuang and  Isabel Leal and  Kuang-Huei Lee and  Sergey Levine and  Yao Lu and  Utsav Malla and  Deeksha Manjunath and  Igor Mordatch and  Ofir Nachum and  Carolina Parada and  Jodilyn Peralta and  Emily Perez and  Karl Pertsch and  Jornell Quiambao and  Kanishka Rao and  Michael Ryoo and  Grecia Salazar and  Pannag Sanketi and  Kevin Sayed and  Jaspiar Singh and  Sumedh Sontakke and  Austin Stone and  Clayton Tan and  Huong Tran and  Vincent Vanhoucke and Steve Vega and  Quan Vuong and  Fei Xia and  Ted Xiao and  Peng Xu and  Sichun Xu and  Tianhe Yu and  Brianna Zitkovich},
    booktitle={arXiv preprint arXiv:2212.06817},
    year={2022}
}</textarea>
                </div>
            </div>
             
        </div>


         <!-- <div class="row">
            <div id="open-source" class="col-md-8 col-md-offset-2">
                <h3>
                    Open Source
                </h3>
              We open source the RL@Scale model <a href="https://github.com/google-research/rl_at_scale">[here]. </a>
              <p style="text-align:center;">
                </p>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    The authors would like to thank Mohi Khansari, Cameron Tuckerman, Stanley Soo, Benjie Holson, Justin Vincent, Mario Prats, Thomas Buschmann, Michael Quinlan, Yunfei Bai, Joséphine Simon, Jarrett Lee, Kalpesh Kuber, Meghha Dhoke, Christian Bodner and Russell Wong for their help and support in various aspects of the project.
                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
